{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial_analytics (2).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dishakhanted/Facial-analytics/blob/master/facial_analytics_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9CWx7JhPmbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0b55b445-606c-47ab-9bc5-8c8eb691129d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KKt4R92PpTZ"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/code\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozTyeJ6LT2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b9f2da8-e4db-4958-be2d-f950f24c96cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rezojtRxlD98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "6621012e-fd02-4c21-ed07-aaa7152bb683"
      },
      "source": [
        "!pip install face_alignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_alignment\n",
            "  Downloading https://files.pythonhosted.org/packages/20/86/26baa3888c254c9ce284702a1041cf9a533ad91c873b06f74d3cfa23aff7/face_alignment-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from face_alignment) (1.5.0+cu101)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from face_alignment) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_alignment) (1.18.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from face_alignment) (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from face_alignment) (4.41.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from face_alignment) (0.16.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->face_alignment) (0.16.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face_alignment) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face_alignment) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face_alignment) (3.2.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face_alignment) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->face_alignment) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->face_alignment) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.12.0)\n",
            "Installing collected packages: face-alignment\n",
            "Successfully installed face-alignment-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRlNgdRnP1oO"
      },
      "source": [
        "use_cuda=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixPOFPQ-P3Qz"
      },
      "source": [
        "import numpy as np\n",
        "import cv2,os\n",
        "import dlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import datasets,hopenet, utils\n",
        "from skimage import io\n",
        "import face_alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147HaRGbMRBR"
      },
      "source": [
        "def landmark_detect(frame_name):\n",
        "  #fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n",
        "  fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False)\n",
        "  #input = io.imread('/home/semantics/2.jpg')\n",
        "  input = io.imread('frame_new.jpg')\n",
        "  preds = fa.get_landmarks(input)\n",
        "  preds = preds[0]\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVo91vejP691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "577a236f-648e-460b-aa37-12ed6f5bef02"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    cudnn.enabled = True\n",
        "    batch_size = 1\n",
        "    gpu = 0\n",
        "    snapshot_path = '/content/drive/My Drive/Colab Notebooks/code/frame_new.jpg'\n",
        "    out_dir = '/content/drive/My Drive/Colab Notebooks/code/output/video'\n",
        "    video_path='/content/drive/My Drive/Colab Notebooks/code/input1.mp4'\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    # if not os.path.exists(args.video_path):\n",
        "    #     sys.exit('Video does not exist')\n",
        "\n",
        "    # ResNet50 structure\n",
        "    model = hopenet.Hopenet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
        "    face_model='/content/drive/My Drive/Colab Notebooks/code/mmod_human_face_detector.dat'\n",
        "    # Dlib face detection model\n",
        "    cnn_face_detector = dlib.cnn_face_detection_model_v1(face_model)\n",
        "\n",
        "    print ('Loading snapshot.')\n",
        "    # Load snapshot\n",
        "   # saved_state_dict = torch.load(snapshot_path)\n",
        "    #model.load_state_dict(saved_state_dict)\n",
        "\n",
        "    print ('Loading data.')\n",
        "\n",
        "    transformations = transforms.Compose([transforms.Scale(224),\n",
        "    transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    model.cuda(gpu)\n",
        "\n",
        "    print ('Ready to test network.')\n",
        "\n",
        "    # Test the Model\n",
        "    model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "    total = 0\n",
        "    idx_tensor = [idx for idx in range(66)]\n",
        "    idx_tensor = torch.FloatTensor(idx_tensor).cuda(gpu)\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    # New cv2\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))   # float\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) # float\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    out = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/code/videof.txt.avi', fourcc, 1, (width, height))\n",
        "\n",
        "    # # Old cv2\n",
        "    # width = int(video.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH))   # float\n",
        "    # height = int(video.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)) # float\n",
        "    #\n",
        "    # # Define the codec and create VideoWriter object\n",
        "    # fourcc = cv2.cv.CV_FOURCC(*'MJPG')\n",
        "    # out = cv2.VideoWriter('output/video/output-%s.avi' % args.output_string, fourcc, 30.0, (width, height))\n",
        "\n",
        "    txt_out = open('/content/drive/My Drive/Colab Notebooks/code/videof.txt', 'w')\n",
        "\n",
        "    #from moviepy.editor import VideoFileClip\n",
        "    #clip = VideoFileClip(\"Video_path\")\n",
        "    #print( clip.duration )\n",
        "\n",
        "    frame_num=1\n",
        "\n",
        "    while frame_num <= 100:\n",
        "        print (\"frame num\",frame_num)\n",
        "        #cv2.imwrite(\"output/\"+str(frame_num)+\".jpg\", frame)\n",
        "        cv2.imwrite(\"frame_new.jpg\", frame)\n",
        "        ret,frame = video.read()\n",
        "        if ret == False:\n",
        "            break\n",
        "\n",
        "\n",
        "        #cv2.imwrite(\"/content/drive/My Drive/Colab Notebooks/code/output/\"+str(frame_num)+\".jpg\", frame)\n",
        "        \n",
        "\n",
        "        cv2_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "#\tcv2.imwrite(\"cnn_face_detection.png\", cv2_frame)\n",
        "#\tcv2_frame = cv2.imread(\"cnn_face_detection.png\")\n",
        "        # Dlib detect\n",
        "        dets = cnn_face_detector(cv2_frame, 1)\n",
        "\n",
        "        for idx, det in enumerate(dets):\n",
        "            # Get x_min, y_min, x_max, y_max, conf\n",
        "            x_min = det.rect.left()\n",
        "            y_min = det.rect.top()\n",
        "            x_max = det.rect.right()\n",
        "            y_max = det.rect.bottom()\n",
        "            conf = det.confidence\n",
        "\n",
        "            if conf > 1.0:\n",
        "                print('yes')\n",
        "                bbox_width = abs(x_max - x_min)\n",
        "                bbox_height = abs(y_max - y_min)\n",
        "                x_min -= 2 * bbox_width / 4\n",
        "                x_max += 2 * bbox_width / 4\n",
        "                y_min -= 3 * bbox_height / 4\n",
        "                y_max += bbox_height / 4\n",
        "                x_min = max(x_min, 0); y_min = max(y_min, 0)\n",
        "                x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "                x_min=int(x_min)\n",
        "                x_max=int(x_max)\n",
        "                y_min=int(y_min)\n",
        "                y_max=int(y_max)\n",
        "                # Crop image\n",
        "                img = cv2_frame[y_min:y_max,x_min:x_max]\n",
        "                img = Image.fromarray(img)\n",
        "\n",
        "                # Transform\n",
        "                img = transformations(img)\n",
        "                img_shape = img.size()\n",
        "                img = img.view(1, img_shape[0], img_shape[1], img_shape[2])\n",
        "                img = Variable(img).cuda(gpu)\n",
        "\n",
        "                yaw, pitch, roll = model(img)\n",
        "\n",
        "                yaw_predicted = F.softmax(yaw)\n",
        "                pitch_predicted = F.softmax(pitch)\n",
        "                roll_predicted = F.softmax(roll)\n",
        "                # Get continuous predictions in degrees.\n",
        "                yaw_predicted = torch.sum(yaw_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "                pitch_predicted = torch.sum(pitch_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "                roll_predicted = torch.sum(roll_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "\n",
        "                # Print new frame with cube and axis\n",
        "                txt_out.write(str(frame_num) + ' %f %f %f\\n' % (yaw_predicted, pitch_predicted, roll_predicted))\n",
        "                # utils.plot_pose_cube(frame, yaw_predicted, pitch_predicted, roll_predicted, (x_min + x_max) / 2, (y_min + y_max) / 2, size = bbox_width)\n",
        "                utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = (x_min + x_max) / 2, tdy= (y_min + y_max) / 2, size = bbox_height/2)\n",
        "#                print (\"writing frame\",frame_num)\n",
        "                cv2.imwrite(\"/content/drive/My Drive/Colab Notebooks/code/output/output\"+str(frame_num)+\".jpg\", frame)\n",
        "\n",
        "                # Plot expanded bounding box\n",
        "                # cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 1)\n",
        "                landmark_coordinates = landmark_detect(frame_num)\n",
        "                res = {}\n",
        "                res['frame_name']=str(frame_num)\n",
        "                res['head_rotation']={}\n",
        "                res['head_rotation']['yaw'] = float(yaw_predicted)\n",
        "                res['head_rotation']['pitch'] = float(pitch_predicted)\n",
        "                res['head_rotation']['roll'] = float(roll_predicted)\n",
        "                res['facial_landmarks']= landmark_coordinates.tolist()\n",
        "                print(res['facial_landmarks'])\n",
        "                with open('./output/combined_'+str(frame_num)+'.json','w') as fitr:\n",
        "                  json.dump(res,fitr)\n",
        "                # Plot expanded bounding box\n",
        "                #cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 1)\n",
        "                \n",
        "        out.write(frame)\n",
        "        frame_num += 1\n",
        "    out.release()\n",
        "    video.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading snapshot.\n",
            "Loading data.\n",
            "Ready to test network.\n",
            "frame num 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "frame num 2\n",
            "frame num 3\n",
            "frame num 4\n",
            "frame num 5\n",
            "frame num 6\n",
            "frame num 7\n",
            "frame num 8\n",
            "frame num 9\n",
            "frame num 10\n",
            "frame num 11\n",
            "frame num 12\n",
            "frame num 13\n",
            "frame num 14\n",
            "frame num 15\n",
            "frame num 16\n",
            "frame num 17\n",
            "frame num 18\n",
            "frame num 19\n",
            "frame num 20\n",
            "frame num 21\n",
            "frame num 22\n",
            "frame num 23\n",
            "frame num 24\n",
            "frame num 25\n",
            "frame num 26\n",
            "frame num 27\n",
            "frame num 28\n",
            "frame num 29\n",
            "frame num 30\n",
            "frame num 31\n",
            "frame num 32\n",
            "frame num 33\n",
            "frame num 34\n",
            "frame num 35\n",
            "frame num 36\n",
            "frame num 37\n",
            "frame num 38\n",
            "frame num 39\n",
            "frame num 40\n",
            "frame num 41\n",
            "frame num 42\n",
            "frame num 43\n",
            "frame num 44\n",
            "frame num 45\n",
            "frame num 46\n",
            "frame num 47\n",
            "frame num 48\n",
            "frame num 49\n",
            "frame num 50\n",
            "frame num 51\n",
            "frame num 52\n",
            "frame num 53\n",
            "frame num 54\n",
            "frame num 55\n",
            "frame num 56\n",
            "frame num 57\n",
            "frame num 58\n",
            "frame num 59\n",
            "frame num 60\n",
            "frame num 61\n",
            "frame num 62\n",
            "frame num 63\n",
            "frame num 64\n",
            "frame num 65\n",
            "frame num 66\n",
            "frame num 67\n",
            "frame num 68\n",
            "frame num 69\n",
            "frame num 70\n",
            "frame num 71\n",
            "frame num 72\n",
            "frame num 73\n",
            "frame num 74\n",
            "frame num 75\n",
            "frame num 76\n",
            "frame num 77\n",
            "frame num 78\n",
            "frame num 79\n",
            "frame num 80\n",
            "frame num 81\n",
            "frame num 82\n",
            "frame num 83\n",
            "frame num 84\n",
            "frame num 85\n",
            "frame num 86\n",
            "frame num 87\n",
            "frame num 88\n",
            "frame num 89\n",
            "frame num 90\n",
            "frame num 91\n",
            "frame num 92\n",
            "frame num 93\n",
            "frame num 94\n",
            "frame num 95\n",
            "frame num 96\n",
            "frame num 97\n",
            "yes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:122: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1088.0, 297.0, -214.11422729492188], [1033.0, 312.0, -169.9148712158203], [978.0, 336.0, -128.4468231201172], [931.0, 352.0, -87.6018295288086], [884.0, 375.0, -36.206092834472656], [861.0, 430.0, 14.909685134887695], [861.0, 485.0, 56.65422821044922], [861.0, 540.0, 98.76904296875], [868.0, 618.0, 121.3671875], [868.0, 681.0, 103.79692840576172], [868.0, 713.0, 65.97899627685547], [884.0, 744.0, 19.433454513549805], [915.0, 768.0, -34.01453399658203], [963.0, 775.0, -81.75713348388672], [1018.0, 783.0, -119.06302642822266], [1080.0, 791.0, -156.4366912841797], [1151.0, 799.0, -198.46510314941406], [1222.0, 407.0, -165.5641632080078], [1253.0, 430.0, -162.4379425048828], [1277.0, 469.0, -156.66278076171875], [1277.0, 501.0, -148.40000915527344], [1277.0, 532.0, -140.37669372558594], [1300.0, 666.0, -131.22946166992188], [1308.0, 697.0, -138.44168090820312], [1300.0, 728.0, -144.4312744140625], [1292.0, 760.0, -149.11968994140625], [1253.0, 783.0, -150.36471557617188], [1222.0, 603.0, -98.27022552490234], [1190.0, 611.0, -65.0768051147461], [1167.0, 611.0, -29.817785263061523], [1143.0, 618.0, -6.397019386291504], [1096.0, 579.0, -22.915197372436523], [1096.0, 595.0, -13.326705932617188], [1088.0, 611.0, -5.7594099044799805], [1096.0, 634.0, -9.092123985290527], [1096.0, 650.0, -15.030097007751465], [1182.0, 461.0, -129.64480590820312], [1206.0, 485.0, -124.46125793457031], [1206.0, 509.0, -125.07057189941406], [1198.0, 540.0, -123.4778823852539], [1190.0, 516.0, -116.28462982177734], [1182.0, 485.0, -117.54711151123047], [1214.0, 666.0, -110.67586517333984], [1229.0, 689.0, -110.25959014892578], [1229.0, 720.0, -110.22733306884766], [1214.0, 744.0, -113.98621368408203], [1198.0, 720.0, -101.94864654541016], [1206.0, 689.0, -102.25735473632812], [1002.0, 532.0, -1.1763439178466797], [1033.0, 563.0, -0.14821307361125946], [1049.0, 603.0, 7.679553031921387], [1049.0, 618.0, 14.423346519470215], [1057.0, 626.0, 14.277681350708008], [1033.0, 666.0, 24.66310691833496], [1002.0, 681.0, 37.5613899230957], [1002.0, 666.0, 55.91378402709961], [994.0, 642.0, 65.81183624267578], [994.0, 618.0, 63.57429504394531], [994.0, 595.0, 55.11835861206055], [994.0, 571.0, 34.032169342041016], [1002.0, 540.0, -4.602078437805176], [1025.0, 595.0, 15.156261444091797], [1033.0, 618.0, 23.106359481811523], [1025.0, 634.0, 28.63467788696289], [1010.0, 681.0, 35.220298767089844], [1025.0, 634.0, 41.18223190307617], [1025.0, 618.0, 36.4542236328125], [1025.0, 595.0, 27.5455265045166]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c8b0e3ee4fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'facial_landmarks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlandmark_coordinates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'facial_landmarks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/combined_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                   \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;31m# Plot expanded bounding box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/combined_97.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hrLoK6VP_Gs"
      },
      "source": [
        "model = hopenet.Hopenet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
        "face_model='/content/drive/My Drive/Colab Notebooks/code/mmod_human_face_detector.dat'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLDQvIrmQY9H"
      },
      "source": [
        "cnn_face_detector = dlib.cnn_face_detection_model_v1(face_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QO-ifZfQlrU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}