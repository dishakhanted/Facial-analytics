{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial_analytics (2).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dishakhanted/Facial-analytics/blob/master/facial_analytics_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9CWx7JhPmbV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KKt4R92PpTZ"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/code\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozTyeJ6LT2K"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rezojtRxlD98"
      },
      "source": [
        "!pip install face_alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRlNgdRnP1oO"
      },
      "source": [
        "use_cuda=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixPOFPQ-P3Qz"
      },
      "source": [
        "import numpy as np\n",
        "import cv2,os\n",
        "import dlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import datasets,hopenet, utils\n",
        "from skimage import io\n",
        "import face_alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147HaRGbMRBR"
      },
      "source": [
        "def landmark_detect(frame_name):\n",
        "  fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False)\n",
        "  #input = io.imread('/home/semantics/2.jpg')\n",
        "  input = io.imread('frame_new.jpg')\n",
        "  preds = fa.get_landmarks(input)\n",
        "  preds = preds[0]\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVo91vejP691"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    cudnn.enabled = True\n",
        "    batch_size = 1\n",
        "    gpu = 0\n",
        "    snapshot_path = '/content/drive/My Drive/Colab Notebooks/code/frame_new.jpg'\n",
        "    out_dir = '/content/drive/My Drive/Colab Notebooks/code/output/video'\n",
        "    video_path='/content/drive/My Drive/Colab Notebooks/code/input1.mp4'\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    # ResNet50 structure\n",
        "    model = hopenet.Hopenet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
        "    face_model='/content/drive/My Drive/Colab Notebooks/code/mmod_human_face_detector.dat'\n",
        "    # Dlib face detection model\n",
        "  \n",
        "    print ('Loading data.')\n",
        "\n",
        "    transformations = transforms.Compose([transforms.Scale(224),\n",
        "    transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    model.cuda(gpu)\n",
        "\n",
        "    print ('Ready to test network.')\n",
        "\n",
        "    # Test the Model\n",
        "    model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "    total = 0\n",
        "    idx_tensor = [idx for idx in range(66)]\n",
        "    idx_tensor = torch.FloatTensor(idx_tensor).cuda(gpu)\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    # New cv2\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))   # float\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) # float\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    out = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/code/videof.txt.avi', fourcc, 1, (width, height))\n",
        "\n",
        "\n",
        "    txt_out = open('/content/drive/My Drive/Colab Notebooks/code/videof.txt', 'w')\n",
        "\n",
        "\n",
        "    frame_num=1\n",
        "\n",
        "    while frame_num <= 100:\n",
        "        print (\"frame num\",frame_num)\n",
        "    \n",
        "        cv2.imwrite(\"frame_new.jpg\", frame)\n",
        "        ret,frame = video.read()\n",
        "        if ret == False:\n",
        "            break\n",
        "\n",
        "\n",
        "        cv2_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "        # Dlib detect\n",
        "        dets = cnn_face_detector(cv2_frame, 1)\n",
        "\n",
        "        for idx, det in enumerate(dets):\n",
        "            # Get x_min, y_min, x_max, y_max, conf\n",
        "            x_min = det.rect.left()\n",
        "            y_min = det.rect.top()\n",
        "            x_max = det.rect.right()\n",
        "            y_max = det.rect.bottom()\n",
        "            conf = det.confidence\n",
        "\n",
        "            if conf > 1.0:\n",
        "                print('yes')\n",
        "                bbox_width = abs(x_max - x_min)\n",
        "                bbox_height = abs(y_max - y_min)\n",
        "                x_min -= 2 * bbox_width / 4\n",
        "                x_max += 2 * bbox_width / 4\n",
        "                y_min -= 3 * bbox_height / 4\n",
        "                y_max += bbox_height / 4\n",
        "                x_min = max(x_min, 0); y_min = max(y_min, 0)\n",
        "                x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "                x_min=int(x_min)\n",
        "                x_max=int(x_max)\n",
        "                y_min=int(y_min)\n",
        "                y_max=int(y_max)\n",
        "                # Crop image\n",
        "                img = cv2_frame[y_min:y_max,x_min:x_max]\n",
        "                img = Image.fromarray(img)\n",
        "\n",
        "                # Transform\n",
        "                img = transformations(img)\n",
        "                img_shape = img.size()\n",
        "                img = img.view(1, img_shape[0], img_shape[1], img_shape[2])\n",
        "                img = Variable(img).cuda(gpu)\n",
        "\n",
        "                yaw, pitch, roll = model(img)\n",
        "\n",
        "                yaw_predicted = F.softmax(yaw)\n",
        "                pitch_predicted = F.softmax(pitch)\n",
        "                roll_predicted = F.softmax(roll)\n",
        "                # Get continuous predictions in degrees.\n",
        "                yaw_predicted = torch.sum(yaw_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "                pitch_predicted = torch.sum(pitch_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "                roll_predicted = torch.sum(roll_predicted.data[0] * idx_tensor) * 3 - 99\n",
        "\n",
        "                # Print new frame with cube and axis\n",
        "                txt_out.write(str(frame_num) + ' %f %f %f\\n' % (yaw_predicted, pitch_predicted, roll_predicted))\n",
        "                utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = (x_min + x_max) / 2, tdy= (y_min + y_max) / 2, size = bbox_height/2)\n",
        "                cv2.imwrite(\"/content/drive/My Drive/Colab Notebooks/code/output/output\"+str(frame_num)+\".jpg\", frame)\n",
        "\n",
        "              \n",
        "                landmark_coordinates = landmark_detect(frame_num)\n",
        "                res = {}\n",
        "                res['frame_name']=str(frame_num)\n",
        "                res['head_rotation']={}\n",
        "                res['head_rotation']['yaw'] = float(yaw_predicted)\n",
        "                res['head_rotation']['pitch'] = float(pitch_predicted)\n",
        "                res['head_rotation']['roll'] = float(roll_predicted)\n",
        "                res['facial_landmarks']= landmark_coordinates.tolist()\n",
        "                print(res['facial_landmarks'])\n",
        "                with open('./output/combined_'+str(frame_num)+'.json','w') as fitr:\n",
        "                  json.dump(res,fitr)\n",
        "          \n",
        "        frame_num += 1\n",
        "    out.release()\n",
        "    video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hrLoK6VP_Gs"
      },
      "source": [
        "model = hopenet.Hopenet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
        "face_model='/content/drive/My Drive/Colab Notebooks/code/mmod_human_face_detector.dat'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLDQvIrmQY9H"
      },
      "source": [
        "cnn_face_detector = dlib.cnn_face_detection_model_v1(face_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}